{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets,models, transforms\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# Just normalize for validation\n",
    "# class torchvision.transforms.RandomResizedCrop(size, scale = (0.08,1.0), ratio = (0.75,1.333333333333), interpolation = 2)\n",
    "# transforms.RandomResizedCrop(224) --> A crop of random size (default of 0.08 to 1.0) of the original size and a \n",
    "# random aspect ratio (default of 3/4 to 4/3) of the original aspect ratio is made\n",
    "# This crop is finally resized to given size (224 in this case)\n",
    "# transforms.CenterCrop(224) --> Crops the image at the center. 224 is the desired output size of the crop\n",
    "\n",
    "# class torchvision.transforms.Normalize(mean,std)\n",
    "# Normalize a tensor image with mean and standard deviation. Given mean: (M1,..., Mn) and std: (S1,...,Sn) for n channels\n",
    "# this transform will normalize each channel of the input torch.Tensor i.e.\n",
    "# output[channel] = (input[channel] - mean[channel]) / std[channel]\n",
    "\n",
    "# Parameters mean (sequence) --->  Sequence of means for each channel\n",
    "#            std (sequence)  --->  Sequence of standard deviations for each channel\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val':  transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the Dataset with labels based on folder names (super important) (also remember to download the dataset from udemy, lesson 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'hymenoptera_data'\n",
    "# Create a dictionary that contains the information of the images in both the training and validation set\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train','val']}\n",
    "# Create a dictionary that contains the data loader\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x],\n",
    "                                              batch_size = 4,\n",
    "                                              shuffle = True) for x in ['train','val']}\n",
    "\n",
    "# Create a dictionary that contains the size of each dataset (training and validation)\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train','val']}\n",
    "# Get the class names\n",
    "class_names = image_datasets['train'].classes\n",
    "# Print out the results \n",
    "print('Class names {}'.format(class_names))\n",
    "print('There are {} batches in the training set'.format(len(dataloaders['train'])))\n",
    "print('There are {} batches in the test set'.format(len(dataloaders['val'])))\n",
    "print('There are {} training images'.format(dataset_sizes['train']))\n",
    "print('There are {} testing images'.format(dataset_sizes['val']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading and Modifying a Pre-Trained Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ResNet\n",
    "model_conv = torchvision.models.resnet18(pretrained = True) # Resnet with 18 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all layers in the network\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False # This prevents backpropagation in all layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of inputs of the last layer (or number of neurons in the layer preceding last layer)\n",
    "num_ftrs = model_conv.fc.in_features # number of inputs features that enter the fully connected part of the resnet\n",
    "# Reconstruct the last layer (output layer) to have only two classes\n",
    "model_conv.fc = nn.Linear(num_ftrs, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the network to the GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    model_conv = model_conv.cuda()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
